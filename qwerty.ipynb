{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ada78c3-857c-4cbd-ab8a-25fcff5de283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U langchain torch transformers sentence-transformers datasets faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e281b10-3a1a-48b2-b80d-09ec58a1e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pdfplumber\n",
    "from datasets import load_dataset\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba08dc3-e54a-46bd-aada-0f4523054f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./p4.pdf\"\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {\"device\": \"mps\" if torch.mps.is_available() else \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "questions = [\n",
    "        \"List of Insured Members?\",\n",
    "        \"What is the Room Rent included in the policy?\",\n",
    "        \"What is the Maternity Sum capping or sum insured?\",\n",
    "        \"What is the policy start date?\",\n",
    "        \"Sum insured for the policy?\",\n",
    "        \"Does the policy have copay?\",\n",
    "        \"what is the policy inception date\",\n",
    "        \"what is the waiting period? and list down the categories for waiting periods\",\n",
    "        \"what is the waiting period for specific disease waiting periods\",\n",
    "        \"what is the waiting period for maternity package \",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1613484-04a6-4185-8f4b-4328e3cd7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_path, max_pages=5):\n",
    "    \"\"\"Extract text and tables from the PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text(layout=True) or \"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2a2be9c-8fab-464f-8a4f-a3514b57e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_id, device):\n",
    "    \"\"\"Initialize or fetch the model and tokenizer.\"\"\"\n",
    "    global cached_model, cached_tokenizer\n",
    "    if not cached_model or not cached_tokenizer:\n",
    "        cached_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        cached_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if device.type == \"mps\" else torch.float32,\n",
    "        ).to(device)\n",
    "    return cached_tokenizer, cached_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9255ec4-1758-48d2-9454-7db7c4a2c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(tokenizer, model, context, question):\n",
    "    \"\"\"Ask a question to the model.\"\"\"\n",
    "    try:\n",
    "        print(f\"Question: {question}\")\n",
    "        input_text = f\"{context}\\n\\nQuestion: {question}\"\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(inputs, max_new_tokens=100)\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer_start = answer.find(\"Answer:\") + len(\"Answer:\")\n",
    "        print(f\"Answer: {answer[answer_start:].strip()}\")\n",
    "        return answer[answer_start:].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during question answering: {e}\")\n",
    "        return \"Unable to generate an answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3084707f-623a-4351-ac9f-95bdc706ade4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './p4.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36mextract_text\u001b[0;34m(pdf_path, max_pages)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract text and tables from the PDF.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpdfplumber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages:\n\u001b[1;32m      6\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mextract_text(layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/chatwithpdf/lib/python3.12/site-packages/pdfplumber/pdf.py:92\u001b[0m, in \u001b[0;36mPDF.open\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, unicode_norm, repair, gs_path, repair_setting)\u001b[0m\n\u001b[1;32m     90\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_fp, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[0;32m---> 92\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_or_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     stream_is_external \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path_or_fp)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './p4.pdf'"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(extract_text(pdf_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468ceec7-611d-4eef-b268-b426fff86885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986ed5a2-c4ff-4d75-b49b-1aa288ae6551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tn/6_4sb_gd0v519hfr8w1k52mm0000gn/T/ipykernel_86006/2175024832.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(\n\u001b[1;32m      2\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodelPath, model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs, encode_kwargs\u001b[38;5;241m=\u001b[39mencode_kwargs\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mdocs\u001b[49m, embeddings)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-7b\", padding=True, truncation=True, max_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c276d79-a835-4f37-8d4e-91737c6656e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      5\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      7\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mbfloat16},\n\u001b[1;32m      8\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt',\n",
    "    max_length=512,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c373292-2df9-4efa-a077-1afa9be73dcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFacePipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m(\n\u001b[1;32m      2\u001b[0m     pipeline\u001b[38;5;241m=\u001b[39mpipe,\n\u001b[1;32m      3\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m},\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HuggingFacePipeline' is not defined"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f17e60d-b56c-4283-affd-304413f0ed1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RetrievalQA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m      2\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m      3\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mdb\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RetrievalQA' is not defined"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55a82b-bf7c-44b8-9a2c-04639fce3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.invoke(\"Write an educational story for young children.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
